{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T13:07:12.200630Z",
     "start_time": "2025-10-16T13:07:10.776179Z"
    }
   },
   "source": [
    "# # Insurance Risk Modeling (Teacher–Student Framework)\n",
    "# **Goal:** Train a high-capacity **teacher** model on **all dataset features** (RandomForest or XGBoost),\n",
    "# then distill it into a **compact student** model that needs **only 4 inputs** at inference time:\n",
    "# `sex`, `weight`, `SMK_stat_type_cd`, `drinker_degree`.\n",
    "#\n",
    "# ⚠️ **Important:** The dataset has **no true actuarial target**. We create a transparent **proxy risk label** from biomedical markers.\n",
    "# Replace it with real insurance outcomes (claims/premium band) for production.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup & Imports\n",
    "\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Optional: XGBoost (teacher); will fallback to RandomForest if not available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "ARTIFACT_DIR = Path('artifacts'); ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Load Data & Basic Inspection\n",
    "\n",
    "FILE = 'smoking_driking_dataset_Ver01.csv'\n",
    "df = pd.read_csv(FILE)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. EDA (quick)\n",
    "\n",
    "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nMissing values (top 15):\\n\", df.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "# Numeric summary\n",
    "summary = df.describe(include='all').T\n",
    "summary.head(20)\n",
    "\n",
    "# Correlation heatmap (normalized)\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "if len(numeric_df.columns) > 1:\n",
    "    scaler = MinMaxScaler()\n",
    "    norm = pd.DataFrame(scaler.fit_transform(numeric_df), columns=numeric_df.columns)\n",
    "    corr = norm.corr()\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(corr, cmap='RdBu_r', center=0, annot=True, fmt='.2f', annot_kws={'size':7}, linewidths=.3)\n",
    "    plt.title('Normalized Feature Correlation Heatmap')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Feature Engineering & Proxy Target\n",
    "# - `drinker_degree` from DRK_YN (0/1)\n",
    "# - BMI from height/weight\n",
    "# - Transparent proxy risk score from biomedical features (HDL protective)\n",
    "# - Convert to 3 bands via quantiles (Low/Medium/High)\n",
    "\n",
    "# Deployment-friendly flags\n",
    "df['drinker_degree'] = (df['DRK_YN'] == 'Y').astype(int)\n",
    "\n",
    "# BMI\n",
    "if {'height','weight'}.issubset(df.columns):\n",
    "    df['BMI'] = df['weight'] / ((df['height']/100.0)**2)\n",
    "else:\n",
    "    df['BMI'] = np.nan\n",
    "\n",
    "# Candidate biomedical features (use only those available)\n",
    "cand_num = [\n",
    "    'age','height','weight','waistline','SBP','DBP','tot_chole','HDL_chole','LDL_chole',\n",
    "    'triglyceride','gamma_GTP','hemoglobin','serum_creatinine','SGOT_AST','SGOT_ALT','BMI'\n",
    "]\n",
    "biomarkers = [c for c in cand_num if c in df.columns]\n",
    "\n",
    "# Build proxy score (z-scored sum; HDL is protective)\n",
    "score = np.zeros(len(df))\n",
    "for col in biomarkers:\n",
    "    x = pd.to_numeric(df[col], errors='coerce')\n",
    "    mu, sd = np.nanmean(x), np.nanstd(x)\n",
    "    z = (x - mu)/(sd if sd and sd>0 else 1.0)\n",
    "    score += (-1.0 if col=='HDL_chole' else 1.0) * np.nan_to_num(z)\n",
    "\n",
    "# Light lifestyle priors\n",
    "score += 0.5 * df['SMK_stat_type_cd'].replace({1:0, 2:0.5, 3:1}).fillna(0)\n",
    "score += 0.25 * df['drinker_degree']\n",
    "\n",
    "df['risk_proxy_score'] = score\n",
    "q_low, q_high = np.nanpercentile(score, [33.3, 66.6])\n",
    "\n",
    "def to_band(s):\n",
    "    if s <= q_low: return 'Low'\n",
    "    if s <= q_high: return 'Medium'\n",
    "    return 'High'\n",
    "\n",
    "df['risk_band'] = df['risk_proxy_score'].apply(to_band)\n",
    "\n",
    "sns.countplot(x='risk_band', data=df, order=['Low','Medium','High'])\n",
    "plt.title('Proxy Risk Bands Distribution')\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Teacher Model — Full Feature Training (XGBoost preferred, else RandomForest)\n",
    "# **Inputs:**\n",
    "# - Categorical: `sex`, `SMK_stat_type_cd`, `drinker_degree`\n",
    "# - Numerical: all biomedical and anthropometric variables present\n",
    "\n",
    "categorical_features = [c for c in ['sex','SMK_stat_type_cd','drinker_degree'] if c in df.columns]\n",
    "numeric_features = [c for c in cand_num if c in df.columns]\n",
    "\n",
    "X_full = df[categorical_features + numeric_features].copy()\n",
    "y = df['risk_band'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Preprocessing: impute + encode/scale\n",
    "preprocess_full = ColumnTransformer([\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), categorical_features),\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numeric_features)\n",
    "])\n",
    "\n",
    "# Class weights via inverse-frequency (for XGB sample_weight)\n",
    "class_counts = y_train.value_counts()\n",
    "class_weight = {cls: len(y_train)/ (len(class_counts)*cnt) for cls, cnt in class_counts.items()}\n",
    "train_sample_weight = y_train.map(class_weight).values\n",
    "\n",
    "# Teacher model selection\n",
    "if XGB_AVAILABLE:\n",
    "    teacher_clf = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        num_class=3,\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    teacher_clf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "teacher_pipe = Pipeline([\n",
    "    ('preprocess', preprocess_full),\n",
    "    ('clf', teacher_clf)\n",
    "])\n",
    "\n",
    "# Fit teacher\n",
    "if XGB_AVAILABLE:\n",
    "    teacher_pipe.fit(X_train, y_train, clf__sample_weight=train_sample_weight)\n",
    "else:\n",
    "    teacher_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation (teacher)\n",
    "y_pred_t = teacher_pipe.predict(X_test)\n",
    "y_proba_t = teacher_pipe.predict_proba(X_test)\n",
    "\n",
    "report_t = pd.DataFrame(classification_report(y_test, y_pred_t, output_dict=True)).T\n",
    "labels = sorted(y.unique())\n",
    "cm_t = confusion_matrix(y_test, y_pred_t, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_t, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Teacher Confusion Matrix')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout(); plt.show()\n",
    "\n",
    "auc_macro_t = roc_auc_score(pd.get_dummies(y_test).reindex(columns=labels, fill_value=0), y_proba_t, multi_class='ovr')\n",
    "print('Teacher Macro ROC-AUC:', round(auc_macro_t, 3))\n",
    "report_t\n",
    "\n",
    "# Save teacher\n",
    "teacher_name = 'xgb' if XGB_AVAILABLE else 'rf'\n",
    "joblib.dump(teacher_pipe, ARTIFACT_DIR / f'risk_model_teacher_{teacher_name}.joblib')\n",
    "with open(ARTIFACT_DIR / f'model_card_teacher_{teacher_name}.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'model': f'Teacher ({\"XGBoost\" if XGB_AVAILABLE else \"RandomForest\"})',\n",
    "        'features_used': categorical_features + numeric_features,\n",
    "        'n_features': len(categorical_features) + len(numeric_features),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'notes': ['Proxy risk label; replace with actuarial target for production.']\n",
    "    }, f, indent=2)\n",
    "print('✅ Teacher saved to artifacts.')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Student Model — 4‑Feature Distillation\n",
    "# **Goal:** Learn a compact model that uses only: `sex`, `weight`, `SMK_stat_type_cd`, `drinker_degree`.\n",
    "# We distill knowledge from the teacher by training the student on teacher predictions (hard labels) and\n",
    "# weighting samples by teacher confidence (max class probability).\n",
    "\n",
    "student_features = [c for c in ['sex','weight','SMK_stat_type_cd','drinker_degree'] if c in df.columns]\n",
    "X_student = df[student_features].copy()\n",
    "\n",
    "# Teacher targets (on entire dataset) for distillation\n",
    "y_proba_teacher_all = teacher_pipe.predict_proba(X_full)\n",
    "y_label_teacher_all = np.array(teacher_pipe.classes_)[y_proba_teacher_all.argmax(axis=1)]\n",
    "y_conf_teacher_all  = y_proba_teacher_all.max(axis=1)\n",
    "\n",
    "# Align split with original y (proxy ground truth used for stratification)\n",
    "Xs_train, Xs_test, yt_train, yt_test, yh_train, yh_test, w_train, w_test = train_test_split(\n",
    "    X_student, y, y_label_teacher_all, y_conf_teacher_all,\n",
    "    test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Student preprocessing\n",
    "student_preprocess = ColumnTransformer([\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), [c for c in student_features if c in ['sex','SMK_stat_type_cd','drinker_degree']]),\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), [c for c in student_features if c in ['weight']])\n",
    "])\n",
    "\n",
    "# Student model: choose a light XGBoost if available, else LogisticRegression\n",
    "if XGB_AVAILABLE:\n",
    "    student_clf = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        num_class=3,\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    student_clf = LogisticRegression(multi_class='multinomial', class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "student_pipe = Pipeline([\n",
    "    ('preprocess', student_preprocess),\n",
    "    ('clf', student_clf)\n",
    "])\n",
    "\n",
    "# Fit student on teacher hard labels, weighted by teacher confidence\n",
    "if XGB_AVAILABLE:\n",
    "    student_pipe.fit(Xs_train, yh_train, clf__sample_weight=w_train)\n",
    "else:\n",
    "    student_pipe.fit(Xs_train, yh_train)\n",
    "\n",
    "# Agreement with teacher\n",
    "yh_pred_test = student_pipe.predict(Xs_test)\n",
    "teacher_agreement = accuracy_score(yh_test, yh_pred_test)\n",
    "print('Teacher–Student agreement (test):', round(teacher_agreement, 3))\n",
    "\n",
    "# Performance vs ground-truth proxy (for reference)\n",
    "ys_pred_test = student_pipe.predict(Xs_test)\n",
    "print('\\nStudent vs proxy ground-truth (test):')\n",
    "print(classification_report(yt_test, ys_pred_test))\n",
    "\n",
    "# Save student\n",
    "student_name = 'xgb' if XGB_AVAILABLE else 'logreg'\n",
    "joblib.dump(student_pipe, ARTIFACT_DIR / f'risk_model_student_4f_{student_name}.joblib')\n",
    "with open(ARTIFACT_DIR / f'model_card_student_4f_{student_name}.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'model': f'Student ({\"XGBoost\" if XGB_AVAILABLE else \"LogisticRegression\"})',\n",
    "        'features_expected': {\n",
    "            'sex': \"['Male','Female']\",\n",
    "            'weight': 'float (kg)',\n",
    "            'SMK_stat_type_cd': 'int [1=never,2=former,3=current]',\n",
    "            'drinker_degree': 'int [0,1]'\n",
    "        },\n",
    "        'notes': ['Student distilled from teacher; inference needs only 4 inputs.']\n",
    "    }, f, indent=2)\n",
    "print('✅ Student saved to artifacts.')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Inference Examples\n",
    "\n",
    "# Choose whichever model you want to serve\n",
    "TEACHER_PATH = ARTIFACT_DIR / f'risk_model_teacher_{teacher_name}.joblib'\n",
    "STUDENT_PATH = ARTIFACT_DIR / f'risk_model_student_4f_{student_name}.joblib'\n",
    "\n",
    "# --- Full model inference (all features) ---\n",
    "def predict_risk_full(**kwargs):\n",
    "    model = joblib.load(TEACHER_PATH)\n",
    "    # Ensure all teacher features are present (fill missing with NaN -> imputed by pipeline)\n",
    "    payload = {**{c: np.nan for c in (categorical_features + numeric_features)}, **kwargs}\n",
    "    X_inf = pd.DataFrame([payload])\n",
    "    pred = model.predict(X_inf)[0]\n",
    "    proba = model.predict_proba(X_inf)[0]\n",
    "    return pred, dict(zip(model.classes_, proba))\n",
    "\n",
    "# --- Minimal 4‑feature inference ---\n",
    "def predict_risk_minimal(sex: str, weight: float, smoker_degree: int, drinker_degree: int):\n",
    "    model = joblib.load(STUDENT_PATH)\n",
    "    X_inf = pd.DataFrame([{\n",
    "        'sex': sex,\n",
    "        'weight': weight,\n",
    "        'SMK_stat_type_cd': smoker_degree,\n",
    "        'drinker_degree': drinker_degree\n",
    "    }])\n",
    "    pred = model.predict(X_inf)[0]\n",
    "    proba = model.predict_proba(X_inf)[0]\n",
    "    return pred, dict(zip(model.classes_, proba))\n",
    "\n",
    "# Examples\n",
    "full_pred, full_proba = predict_risk_full(\n",
    "    sex='Female', age=40, height=165, weight=60, waistline=75, SBP=120, DBP=80,\n",
    "    tot_chole=180, HDL_chole=60, LDL_chole=110, triglyceride=100, gamma_GTP=25,\n",
    "    hemoglobin=14, serum_creatinine=0.8, SGOT_AST=25, SGOT_ALT=20,\n",
    "    SMK_stat_type_cd=1, drinker_degree=0\n",
    ")\n",
    "print('[Teacher] Pred:', full_pred); print('Proba:', full_proba)\n",
    "\n",
    "stud_pred, stud_proba = predict_risk_minimal('Male', 82.0, 3, 1)\n",
    "print('\\n[Student 4f] Pred:', stud_pred); print('Proba:', stud_proba)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Explainability (quick view)\n",
    "# For tree models (RF/XGB), built-in feature importance (global). For the student LogReg, coefficients.\n",
    "\n",
    "try:\n",
    "    # Extract preprocessed feature names\n",
    "    ohe = teacher_pipe.named_steps['preprocess'].named_transformers_['cat'].named_steps['ohe']\n",
    "    cat_names = ohe.get_feature_names_out(categorical_features)\n",
    "    num_names = numeric_features\n",
    "    feat_names = np.concatenate([cat_names, num_names])\n",
    "\n",
    "    clf = teacher_pipe.named_steps['clf']\n",
    "    if XGB_AVAILABLE:\n",
    "        importances = clf.feature_importances_\n",
    "    else:\n",
    "        importances = clf.feature_importances_\n",
    "\n",
    "    imp_df = pd.DataFrame({'feature': feat_names, 'importance': importances}).sort_values('importance', ascending=False).head(25)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.barplot(data=imp_df, x='importance', y='feature')\n",
    "    plt.title('Top Feature Importances (Teacher)')\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print('Explainability plot skipped:', e)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Conclusions & Next Steps\n",
    "# - The **teacher** (XGBoost/RandomForest) leverages **all clinical features** and provides higher discriminative power.\n",
    "# - The **student** is distilled to use **only 4 inputs** for deployment, achieving good agreement with the teacher.\n",
    "# - Replace the proxy target with **real actuarial labels** for pricing/underwriting.\n",
    "# - Add **calibration** (e.g., isotonic) and **fairness checks** by sex/age.\n",
    "# - Monitor drift of input distributions and risk share over time.\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 138\u001B[0m\n\u001B[1;32m    124\u001B[0m preprocess \u001B[38;5;241m=\u001B[39m ColumnTransformer(\n\u001B[1;32m    125\u001B[0m     transformers\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m    126\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcat\u001B[39m\u001B[38;5;124m\"\u001B[39m, OneHotEncoder(handle_unknown\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m), cat_features),\n\u001B[1;32m    127\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum\u001B[39m\u001B[38;5;124m\"\u001B[39m, StandardScaler(), num_features),\n\u001B[1;32m    128\u001B[0m     ]\n\u001B[1;32m    129\u001B[0m )\n\u001B[1;32m    131\u001B[0m base_clf \u001B[38;5;241m=\u001B[39m LogisticRegression(\n\u001B[1;32m    132\u001B[0m     multi_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultinomial\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    133\u001B[0m     class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    134\u001B[0m     max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,\n\u001B[1;32m    135\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mRANDOM_STATE,\n\u001B[1;32m    136\u001B[0m )\n\u001B[0;32m--> 138\u001B[0m clf \u001B[38;5;241m=\u001B[39m \u001B[43mCalibratedClassifierCV\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_clf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43misotonic\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m pipe \u001B[38;5;241m=\u001B[39m Pipeline(steps\u001B[38;5;241m=\u001B[39m[(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpreprocess\u001B[39m\u001B[38;5;124m\"\u001B[39m, preprocess), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclf\u001B[39m\u001B[38;5;124m\"\u001B[39m, clf)])\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# --- Train ---\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
